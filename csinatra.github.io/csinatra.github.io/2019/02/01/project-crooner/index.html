<!DOCTYPE html><html lang="en"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><title>Crooner: A Love Song Lyric Generator | Chris Sinatra</title><meta name="description" content="A collection of Data Science projects by Chris Sinatra"><meta itemprop="name" content="Chris Sinatra"><meta itemprop="description" content="The problem of de novo text generation has direct application in fields such as social media, content generation, direct response marketing, predictive text, machine translation, and customer service automation. In this project, I built love-song lyric generator trained on a real-world dataset, returning entirely new song lyrics based on a user generated seed text."><meta itemprop="image" content="https://chrissinatra.dev/assets/images/crooner.svg"><meta property="og:url" content="https://chrissinatra.dev/2019/02/01/project-crooner/"><meta property="og:type" content="website"><meta property="og:title" content="Crooner: A Love Song Lyric Generator | Chris Sinatra"><meta property="og:site_name" content="Chris Sinatra"><meta property="og:description" content="The problem of de novo text generation has direct application in fields such as social media, content generation, direct response marketing, predictive text, machine translation, and customer service automation. In this project, I built love-song lyric generator trained on a real-world dataset, returning entirely new song lyrics based on a user generated seed text."><meta property="og:image" content="https://chrissinatra.dev/assets/images/crooner.svg"><meta name="twitter:url" content="https://chrissinatra.dev/2019/02/01/project-crooner/"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Crooner: A Love Song Lyric Generator | Chris Sinatra"><meta name="twitter:site" content="Chris Sinatra"><meta name="twitter:description" content="The problem of de novo text generation has direct application in fields such as social media, content generation, direct response marketing, predictive text, machine translation, and customer service automation. In this project, I built love-song lyric generator trained on a real-world dataset, returning entirely new song lyrics based on a user generated seed text."><meta property="twitter:image" content="https://chrissinatra.dev/assets/images/crooner.svg"><link rel="icon" type="image/x-icon" href="/assets/images/favicon.ico"><link rel="stylesheet" href="/assets/css/app.min.css"><link rel="alternate" type="application/rss+xml" title="Chris Sinatra" href="/feed.xml"><link rel="canonical" href="/2019/02/01/project-crooner/"> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-28652711-1"></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-28652711-1'); </script></head><body id="crooner-a-love-song-lyric-generator" class="post-layout"><header class="header"> <a class="header__title" href="https://chrissinatra.dev/">Chris Sinatra</a><nav><ul class="header__list"><li><a href="/">Projects</a></li><li><a href="/about">About</a></li><li><span class="popup__open">Contact</span></li></ul></nav></header><main class="ðŸ’ˆ"><div class="post"><article itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting"><div class="post__header section-padding--double"><div class="grid-small"><h2 itemprop="name headline">Crooner: A Love Song Lyric Generator</h2><time class="post__date" datetime="2019-02-01T00:00:00-08:00" itemprop="datePublished">1 Feb 2019</time></div></div><div class="post__img"><div><figure class="absolute-bg" style="background-image: url('/assets/images/crooner.svg');"></figure></div></div><div class="post__content section-padding"><div class="grid"><div id="markdown" itemprop="articleBody"><p>The problem of de novo text generation has direct application in fields such as social media, content generation, direct response marketing, predictive text, machine translation, and customer service automation. To explore this space, I built love-song lyric generator trained on a real-world dataset, returning entirely new song lyrics based on a user generated seed text. Data was collected from Spotify and Genius lyrics and modeled with a LSTM Neural Network to create entirely new songs.</p><p>This may also serve as a foundational underpinning for complete song or voice generation models which may be used across a variety of Enterprise applications.</p><h2 id="problem-statement">Problem Statement<a name="section1"></a></h2><p>This project aims to build a word-level love-song lyric generator trained on a real-world dataset made up of existing tracks and returning entirely new song lyrics based on a user generated seed text.</p><p>The problem of de novo text generation has direct application in fields such as social media content generation, direct response marketing, predictive text generation, or text-based customer service correspondence. This may also serve as a foundational underpinning for complete song or voice generation models which may be used across a variety of Enterprise applications.</p><h2 id="executive-summary">Executive Summary<a name="section2"></a></h2><p>Exploring word-level models based on an LSTM neural network architecture, my model was able to achieve 86.9% training accuracy and generate convincing lyrics that preserve basic song structure.</p><p>Here is an example song generated with the seed â€˜eyes are for loversâ€™: <img src="/assets/images/m1_eyes.png" alt="eyes are for lovers" /></p><p>The dataset was collected by querying the Spotify API <a href="https://developer.spotify.com/documentation/web-api/">(source)</a> for Spotify Curated playlist track lists from the Romance genre and then scraping the Genius lyric database <a href="https://genius.com/developers">(source)</a> to build a corpus of 1781 tracks and 12334 unique words.</p><p>After cleaning the data, training and optimizing a LSTM model and generating sample lyrics, the model was deployed in a console application as well as proof of concept Flask app.</p><h2 id="background">Background<a name="section3"></a></h2><p>Predictive text generation models have been deployed by companies such as Google with wide adoption. Current production models are commonly built upon neural network architectures such as Convolutional Neural Nets (CNNs) or more recently Recurrent Neural Networks (RNNs) using character-level predictions.</p><p>While RNNs have been shown to produce strong results for more shallow networks and shorter texts, their performance tends to drop off with deeper architectures. This is due to a property dubbed the vanishing gradient problem, which describes an issue with downstream layers losing the ability to tune the weights of earlier layers. To address this issue, Long Short Term Memory (LSTM) networks allow samples to pass directly through to downstream layers, thereby preserving a direct path for tuning via forward and back-propagation. <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">(source)</a></p><p><img src="/assets/images/rnn_lstm.jpg" alt="rnn vs lstm" /> <em>An unrolled RNN (top) vs LSTM (bottom) <a href="https://www.oreilly.com/ideas/introduction-to-lstms-with-tensorflow">(source)</a></em></p><p>A single LSTM cell has 3 states: a forget gate, a memory gate, and a sigmoid output layer. The first layer determines what information is kept from previous states, the next layer determines what new information will be added to the cell, and the final layer determines what information will be passed to the next LSTM cell. In this way, LSTM cells are able to avoid the problem of the vanishing gradient.</p><p>Leveraging the deep learning libraries of Keras running on a Tensorflow backend, a dataset of 1781 songs was split into 567142 training sequences and passed through the model. The songs were collected from the Romance category of Spotify Curated playlists. Each playlist is between 15-100 tracks and is professionally selected by Spotify Staff. Lyrics were scraped from Genius, one of the largest lyric databases populated by moderated user annotated lyrics. Once collected, the data was pushed to a SQL database for possible use as a backend server.</p><p>Song lyrics were cleaned and split into individual words for each track before processing and being fed into the model as tokenized word-level sequence sets. Predictions based on these input sets were validated against the ground truth next word in the track and the models were scored based on accuracy.</p><h2 id="data-dictionary">Data Dictionary<a name="section4"></a></h2><p>SQL Tables:</p><style> table, th, td { border: 1px solid black; }</style><p><strong>track_table:</strong></p><table><thead><tr><th>Feature</th><th>SQL Type</th><th>Data Type</th><th>Description</th></tr></thead><tbody><tr><td>track_id</td><td>VARCHAR / PRIMARY KEY</td><td>string</td><td>Unique track ID assigned by Spotify and used to trace back to song metadata throughout this project.</td></tr><tr><td>playlist_id</td><td>VARCHAR</td><td>string</td><td>Unique playlist ID assigned by Spotify and used to group tracks by playlist.</td></tr><tr><td>track_name</td><td>VARCHAR</td><td>string</td><td>Name of the Track.</td></tr><tr><td>artist_name</td><td>VARCHAR</td><td>string</td><td>Name of the Artist.</td></tr><tr><td>album_name</td><td>VARCHAR</td><td>string</td><td>Name of the Album.</td></tr><tr><td>playlist_name</td><td>VARCHAR</td><td>string</td><td>Name of the Spotify playlist.</td></tr><tr><td>playlist_owner</td><td>VARCHAR</td><td>string</td><td>Name of the playlist creator (Spotify).</td></tr><tr><td>lyrics</td><td>JSON</td><td>string</td><td>Lyrics queried from Genius (http://genius.com)</td></tr></tbody></table><p><strong>lyric_table:</strong></p><table><thead><tr><th>Feature</th><th>SQL Type</th><th>Data Type</th><th>Description</th></tr></thead><tbody><tr><td>track_id</td><td>VARCHAR / PRIMARY KEY</td><td>string</td><td>Unique track ID assigned by Spotify and used to trace back to song metadata throughout this project.</td></tr><tr><td>rep_ratio</td><td>FLOAT</td><td>float</td><td>Ratio of unique words to total words in the track</td></tr><tr><td>total_words_track</td><td>INT</td><td>int</td><td>Word count for the track</td></tr><tr><td>unique_words_track</td><td>INT</td><td>int</td><td>Unique word count for the track</td></tr><tr><td>mean_len_words_track</td><td>FLOAT</td><td>float</td><td>Mean word length for the track</td></tr><tr><td>total_lines_track</td><td>INT</td><td>int</td><td>Line count for the track</td></tr><tr><td>unique_lines_track</td><td>INT</td><td>int</td><td>Unique line count for the track</td></tr><tr><td>mean_words_line</td><td>FLOAT</td><td>float</td><td>Mean word count per line in the track</td></tr><tr><td>mean_unique_words_line</td><td>FLOAT</td><td>float</td><td>Mean unique word count per line in the track</td></tr></tbody></table><h2 id="findings">Findings<a name="section5"></a></h2><p>Creating a python-based word-level text generator, I decided to implement a sliding window sequence to feed data into the model. This means that an input sequence of <em>m</em> words will be used to generate an output sequence of <em>n</em> words. Sequencing the data in this way has the benefit of providing a ground truth feature that can be used to score model accuracy during training.</p><p>Based on song structure findings during EDA, I decided on an input sequence of four words, an output sequence of one word, and a 250 word song length. This corresponds to an average of 9.1 words/line and 282.6 words/track seen in the dataset. I also chose to keep â€˜\nâ€™ characters in an effort to preserve song structure.</p><p><img src="/assets/images/words_line.png" alt="mean words per line" /></p><p><img src="/assets/images/words_track.png" alt="words per track" /></p><p>Based on track length, I decided to build my model with an embedding layer followed by a first LSTM layer of 300 nodes, corresponding to roughly 1 node per word in the track. This feeds into a second LSTM layer with 150 nodes, followed by a Dense layer with 100 nodes and finally an output layer with the same shape as the training corpus.</p><p>The best results were produced when using a model trained over 300 epochs with batches of 5000 input sequences and untrained word word vectors with 3000 dimensions. The optimization function was the ADAM optimizer and the loss function was categorical cross-entropy.</p><p>The model architecture was as follows:</p><ul><li>Keras Embedding (3000)</li><li>LSTM (300, return_sequences=True)</li><li>LSTM (150)</li><li>Dense (100, ReLu)</li><li>Dense (vocab_size, softmax)</li></ul><p>The Keras embedding layer is randomly initialized and fit alongside the model during training. Since I started with a vocabulary of 12334 unique words it seemed reasonable to begin with reducing the input dimensionality down to a 3000 feature dense representation. I also explored the use of pre-trained word vectors from the GloVe Common Crawl 42B dataset for comparison <a href="https://nlp.stanford.edu/projects/glove/">(source)</a>.</p><p>Lyrics produced from a model fit with pre-trained word vectors from the GloVe 42B dataset were more likely to repeat or fall into loops, requiring a larger variance coefficient during lyric generation to avoid repetition. This may indicated a need for more training epochs or a deeper overall model architecture. Since the embeddings were not a parameter being fit by the model, it is also possible that allowing the model to update embedding weights would also lead to better results. Line structure was also markedly less robust, often being choppy and 4-6 words compared to the longer lines of other models.</p><p>To inject a tunable amount of variance in the model predictions, predicted probabilities for the next word were scaled and then randomly sampled from a distribution. This technique, adopted from the Keras LSTM example framework, prevents predictions from falling into a loop of high likelihood word patterns or into lyrics that the model was initially trained on <a href="https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py">(source)</a>.</p><p>This framework produced 250 word lyric sets that showed minimal repetition, maintained human readability, and preserved an output structure that could pass as real-world lyrics.</p><p>Reviewing song structure as a repetition grid, here is the output for the model generated track from above:</p><p><img src="/assets/images/eyes_grid.png" alt="eyes_grid" /> <em>Model-generated song repetition grid without pre-trained embeddings</em></p><p><img src="/assets/images/always_grid.png" alt="eyes_grid" /> <em>Model-generated song repetition grid with pre-trained embeddings</em></p><p><img src="/assets/images/always_be.png" alt="eyes_grid" /> <em>Lyrics generated from the seed â€˜always be there loveâ€™</em></p><p>In this grid, the vertical and horizontal axis represent each word in the track. If a word is repeated, the cell is filled, otherwise it is left white. This can be compared to a real track below. Note that the song below has about 30% higher word repetition (~ 0.64, average for the dataset compared to 0.39 above). It is possible that as the model becomes better optimized these structural components will become more well-defined.</p><p><img src="/assets/images/teddy_grid.png" alt="real_grid" /><em>Real song repetition grid</em></p><p><img src="/assets/images/teddy.png" alt="teddy" /><em>Lyrics for When Somebody Loves You Back, Teddy Pendergrass <a href="https://genius.com/Teddy-pendergrass-when-somebody-loves-you-back-lyrics">(source)</a></em></p><h2 id="business-recommendations">Business Recommendations<a name="section6"></a></h2><p>Based on my findings, itâ€™s clear that LSTM models provide a solid architecture for language processing and text generation. This model is robust enough to produce entertaining results that can be deployed in a consumer level product with minimal additional front-end development.</p><p>This work provides a foundation for further exploration of word- and document-level language processing frameworks which can be applied to use-cases such as translation, marketing / copywriting, document summarization, image captioning, and other areas where automated speech and text may be beneficial.</p><p>In the context of personalized, targeted marketing where brand voice and authenticity are paramount, a model such as this one may be able to provide basic copy that requires minimal human-in-the-loop feedback to become a market-ready document that adheres to pre-established and longstanding brand guidelines.</p><h2 id="next-steps">Next steps<a name="section7"></a></h2><p>This model would benefit from a more exhaustive optimization of both architecture and hyperparameter tuning. To accomplish this will require scaling up compute.</p><p>One tool that I would be interested in exploring is the Adanet adaptive architecture package that can dynamically test the addition of layers and nodes to create better predictions. I would have also liked to have explored pre-trained word vectors with longer training runs as Iâ€™m confident that a better semantic understanding of the data would result in more humanistic predictions.</p><p>I would also like to build out the model with a more consumer-facing interface alongside the lyric repetition grid and interactive versions of other song structure plots produced during EDA.</p><p>A natural extension of this project would be training on a larger dataset that is grouped by genre to see how well the model can mimic not only syntax, but also sentiment according to well conserved song structures.</p><p>Another interesting idea would be training the model on company websites, press releases, and other public-facing documents to see how well the model can adopt brand voice.</p><h2 id="conclusion">Conclusion<a name="section8"></a></h2><p>It is clear this model has great potential for language processing and text generation. For a word-level model to produce these results on a relatively small dataset is frankly quite remarkable. As an exploratory project seeking to demonstrate the potential of LSTM architectures for generating human-readable text, this is a tremendous starting point. Consumer and enterprise-level use-cases for this functionality are broad and thereâ€™s no doubt that machine-generated language processing will only become more prevalent in the coming years.</p><p style="text-align: center;"><b>Full project repo is available on <a href="https://github.com/csinatra/portfolio/tree/master/crooner" target="_blank">GitHub</a></b></p></div><ul class="post__social"><li><a href="https://www.facebook.com/sharer/sharer.php?u=https://chrissinatra.dev/2019/02/01/project-crooner/" target="_blank"><i class="fa fa-facebook"></i></a></li><li><a href="https://twitter.com/intent/tweet?&text=Crooner: A Love Song Lyric Generator+https://chrissinatra.dev/2019/02/01/project-crooner/+by+Chris Sinatra" target="_blank"><i class="fa fa-twitter"></i></a></li><li><a href="https://plus.google.com/share?url=https://chrissinatra.dev/2019/02/01/project-crooner/" target="_blank"><i class="fa fa-google-plus"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?mini=true&source=Crooner: A Love Song Lyric Generator&summary=The problem of de novo text generation has direct application in fields such as social media, content generation, direct response marketing, predictive text, machine translation, and customer service automation. In this project, I built love-song lyric generator trained on a real-world dataset, returning entirely new song lyrics based on a user generated seed text.&url=https://chrissinatra.dev/2019/02/01/project-crooner/" target="_blank"><i class="fa fa-linkedin"></i></a></li><li><a href="https://www.stumbleupon.com/badge/?url=https://chrissinatra.dev/2019/02/01/project-crooner/" target="_blank"><i class="fa fa-stumbleupon"></i></a></li><li><a href="https://www.reddit.com/submit?url=https://chrissinatra.dev/2019/02/01/project-crooner/" target="_blank"><i class="fa fa-reddit-alien"></i></a></li><li><a href="https://www.tumblr.com/share/link?url=https://chrissinatra.dev/2019/02/01/project-crooner/" target="_blank"><i class="fa fa-tumblr"></i></a></li><li><a href="https://www.pinterest.com/pin/create/link/?description=The problem of de novo text generation has direct application in fields such as social media, content generation, direct response marketing, predictive text, machine translation, and customer service automation. In this project, I built love-song lyric generator trained on a real-world dataset, returning entirely new song lyrics based on a user generated seed text.&media=https://chrissinatra.dev/assets/images/crooner.svg&url=https://chrissinatra.dev/2019/02/01/project-crooner/" target="_blank"><i class="fa fa-pinterest"></i></a></li></ul></div></div><div class="section-padding--none"><div class="grid"><hr class="sep"/></div></div><div class="section-padding"><div class="grid-small"> <span class="post__author">Posted by <a href="https://chrissinatra.dev" title="More By Chris Sinatra">Chris Sinatra</a></span><p class="post__bio">I'm a Data Scientist living in Santa Monica, CA. Driven by curiosity, I'm eager to apply ML and Data Science techniques to create scalable, robust solutions to complex problems.</p></div></div></article></div><section class="related section-padding"><div class="grid-xlarge"><h2 class="related__title">Related</h2><div class="related__container"><article class="related__post"> <a class="related__link" href="https://chrissinatra.dev/2019/02/01/project-crooner/"><figure class="related__img"> <img src="/assets/images/crooner.svg" alt="Crooner: A Love Song Lyric Generator"/></figure><div><h2 class="related__text">Crooner: A Love Song Lyric Generator</h2></div></a></article><article class="related__post"> <a class="related__link" href="https://chrissinatra.dev/2019/01/01/project-nlt/"><figure class="related__img"> <img src="/assets/images/nlt_cover.svg" alt="New Light Technologies: Disaster Locator"/></figure><div><h2 class="related__text">New Light Technologies: Disaster Locator</h2></div></a></article></div></div></section></main><footer class="footer section-padding"><div class="grid"><div class="subscribe" id="subscribe"><div class="subscribe__container"> <span class="subscribe__title">Subscribe</span><p class="subscribe__text">Get a monthly email of posts Iâ€™ve added to the site.</p><form method="POST" action="&amp;c=?" id="mc-signup" name="mc-embedded-subscribe-form" novalidate><div style="position: absolute; left: -5000px;" aria-hidden="true"> <input type="text" name="" tabindex="-1" value=""></div><div class="form-group"> <input id="mce-EMAIL" type="email" name="EMAIL" placeholder="Email Address"></div><div class="form__btn"> <input id="mc-submit" type="submit" value="Sign Up" name="subscribe"></div></form><p class="subscribe__error hidden"></p></div></div><hr class="sep--white"/><div class="footer__container"><ul class="footer__tags"><li><a class="footer__link" href="/tag/nlp">Nlp</a></li><li><a class="footer__link" href="/tag/projects">Projects</a></li><li><a class="footer__link" href="/tag/svm">Svm</a></li><li><a class="footer__link" href="/tag/knn">Knn</a></li><li><a class="footer__link" href="/tag/scd">Scd</a></li></ul><ul class="footer__social"><li><a href="https://www.linkedin.com/in/chrissinatra" target="_blank"><i class="fa fa-linkedin"></i></a></li><li><a href="https://github.com/csinatra/portfolio" target="_blank"><i class="fa fa-github"></i></a></li><li><a href="https://www.instagram.com/csinatra" target="_blank"><i class="fa fa-instagram"></i></a></li><li><a href="https://drive.google.com/file/d/1FFqjOrZ-XBsPQGakl0r6JZC5D3Kbl5qr/view?usp=sharing" target="_blank"><i class="fa fa-google-drive"></i></a></li></ul></div></div></footer><section class="contact popup"><div class="popup__close"><div class="popup__exit"></div></div><div class="contact__container popup__container"><div class="contact__img"><figure class="absolute-bg" style="background-image: url(/assets/images/20200525_profile.jpg);"></figure></div><div class="contact__content"><div class="contact__mast section-padding--half"><div class="grid"><h2>Contact</h2></div></div><div class="section-padding--none"><hr class="sep"/></div><div class="contact__form section-padding--half"><div class="grid-xlarge"> <form id="form" class="form" action="https://formcarry.com/s/hM9LL4fr4rL" method="POST"><div class="form__subcontainer"><div> <label for="form-first-name">First Name</label> <input type="text" name="first-name" id="form-first-name" required></div><div> <label for="form-last-name">Last Name</label> <input type="text" name="last-name" id="form-last-name" required></div></div><div> <label for="form-email">E-Mail</label> <input type="email" name="email" id="form-email" required></div><div> <label for="form-message">Message</label> <textarea name="message" id="form-message" rows="3"></textarea></div><div class="form__submit"><div class="form__btn"> <input type="submit" value="Send"></div></div><p class="form__message"></p></form></div></div></div></div></section><script src="/assets/js/app.min.js"></script></body></html>
  