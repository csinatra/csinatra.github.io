<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Chris Sinatra</title>
    <description>A collection of Data Science projects by Chris Sinatra
</description>
    <link>https://chrissinatra.dev</link>
    <atom:link href="https://chrissinatra.dev/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 24 Mar 2019 12:28:45 -0700</pubDate>
    <lastBuildDate>Sun, 24 Mar 2019 12:28:45 -0700</lastBuildDate>
    <generator>Jekyll v3.8.5</generator>
    
      <item>
        <title>Crooner: A Love Song Lyric Generator</title>
        <description>&lt;p&gt;The problem of de novo text generation has direct application in fields such as social media content generation, direct response marketing, predictive text generation, or text-based customer service correspondence. To explore this space, I built love-song lyric generator trained on a real-world dataset, returning an entirely new song lyrics based on a user generated seed text. Data was collected from Spotify and Genius lyrics and modeled with a LSTM Neural Network to create entirely new songs.&lt;/p&gt;

&lt;p&gt;This may also serve as a foundational underpinning for complete song or voice generation models which may be used across a variety of Enterprise applications.&lt;/p&gt;

&lt;h2 id=&quot;problem-statement&quot;&gt;Problem Statement&lt;a name=&quot;section1&quot;&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This project aims to build a word-level love-song lyric generator trained on a real-world dataset made up of existing tracks and returning entirely new song lyrics based on a user generated seed text.&lt;/p&gt;

&lt;p&gt;The problem of de novo text generation has direct application in fields such as social media content generation, direct response marketing, predictive text generation, or text-based customer service correspondence. This may also serve as a foundational underpinning for complete song or voice generation models which may be used across a variety of Enterprise applications.&lt;/p&gt;

&lt;h2 id=&quot;executive-summary&quot;&gt;Executive Summary&lt;a name=&quot;section2&quot;&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Exploring word-level models based on an LSTM neural network architecture, my model was able to achieve 86.9% training accuracy and generate convincing lyrics that preserve basic song structure.&lt;/p&gt;

&lt;p&gt;Here is an example song generated with the seed ‘eyes are for lovers’:
&lt;img src=&quot;/assets/images/m1_eyes.png&quot; alt=&quot;eyes are for lovers&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The dataset was collected by querying the Spotify API &lt;a href=&quot;https://developer.spotify.com/documentation/web-api/&quot;&gt;(source)&lt;/a&gt; for Spotify Curated playlist track lists from the Romance genre and then scraping the Genius lyric database &lt;a href=&quot;https://genius.com/developers&quot;&gt;(source)&lt;/a&gt; to build a corpus of 1781 tracks and 12334 unique words.&lt;/p&gt;

&lt;p&gt;After cleaning the data, training and optimizing a LSTM model and generating sample lyrics, the model was deployed in a console application as well as proof of concept Flask app.&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;a name=&quot;section3&quot;&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Predictive text generation models have been deployed by companies such as Google with wide adoption. Current production models are commonly built upon neural network architectures such as Convolutional Neural Nets (CNNs) or more recently Recurrent Neural Networks (RNNs) using character-level predictions.&lt;/p&gt;

&lt;p&gt;While RNNs have been shown to produce strong results for more shallow networks and shorter texts, their performance tends to drop off with deeper architectures. This is due to a property dubbed the vanishing gradient problem, which describes an issue with downstream layers losing the ability to tune the weights of earlier layers. To address this issue, Long Short Term Memory (LSTM) networks allow samples to pass directly through to downstream layers, thereby preserving a direct path for tuning via forward and back-propagation. &lt;a href=&quot;http://colah.github.io/posts/2015-08-Understanding-LSTMs/&quot;&gt;(source)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/rnn_lstm.jpg&quot; alt=&quot;rnn vs lstm&quot; /&gt; &lt;em&gt;An unrolled RNN (top) vs LSTM (bottom) &lt;a href=&quot;https://www.oreilly.com/ideas/introduction-to-lstms-with-tensorflow&quot;&gt;(source)&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;A single LSTM cell has 3 states: a forget gate, a memory gate, and a sigmoid output layer. The first layer determines what information is kept from previous states, the next layer determines what new information will be added to the cell, and the final layer determines what information will be passed to the next LSTM cell. In this way, LSTM cells are able to avoid the problem of the vanishing gradient.&lt;/p&gt;

&lt;p&gt;Leveraging the deep learning libraries of Keras running on a Tensorflow backend, a dataset of 1781 songs was split into 567142 training sequences and passed through the model. The songs were collected from the Romance category of Spotify Curated playlists. Each playlist is between 15-100 tracks and is professionally selected by Spotify Staff. Lyrics were scraped from Genius, one of the largest lyric databases populated by moderated user annotated lyrics. Once collected, the data was pushed to a SQL database for possible use as a backend server.&lt;/p&gt;

&lt;p&gt;Song lyrics were cleaned and split into individual words for each track before processing and being fed into the model as tokenized word-level sequence sets. Predictions based on these input sets were validated against the ground truth next word in the track and the models were scored based on accuracy.&lt;/p&gt;

&lt;h2 id=&quot;data-dictionary&quot;&gt;Data Dictionary&lt;a name=&quot;section4&quot;&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;SQL Tables:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;track_table:&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Feature&lt;/th&gt;
      &lt;th&gt;SQL Type&lt;/th&gt;
      &lt;th&gt;Data Type&lt;/th&gt;
      &lt;th&gt;Description&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;track_id&lt;/td&gt;
      &lt;td&gt;VARCHAR / PRIMARY KEY&lt;/td&gt;
      &lt;td&gt;string&lt;/td&gt;
      &lt;td&gt;Unique track ID assigned by Spotify and used to trace back to song metadata throughout this project.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;playlist_id&lt;/td&gt;
      &lt;td&gt;VARCHAR&lt;/td&gt;
      &lt;td&gt;string&lt;/td&gt;
      &lt;td&gt;Unique playlist ID assigned by Spotify and used to group tracks by playlist.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;track_name&lt;/td&gt;
      &lt;td&gt;VARCHAR&lt;/td&gt;
      &lt;td&gt;string&lt;/td&gt;
      &lt;td&gt;Name of the Track.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;artist_name&lt;/td&gt;
      &lt;td&gt;VARCHAR&lt;/td&gt;
      &lt;td&gt;string&lt;/td&gt;
      &lt;td&gt;Name of the Artist.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;album_name&lt;/td&gt;
      &lt;td&gt;VARCHAR&lt;/td&gt;
      &lt;td&gt;string&lt;/td&gt;
      &lt;td&gt;Name of the Album.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;playlist_name&lt;/td&gt;
      &lt;td&gt;VARCHAR&lt;/td&gt;
      &lt;td&gt;string&lt;/td&gt;
      &lt;td&gt;Name of the Spotify playlist.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;playlist_owner&lt;/td&gt;
      &lt;td&gt;VARCHAR&lt;/td&gt;
      &lt;td&gt;string&lt;/td&gt;
      &lt;td&gt;Name of the playlist creator (Spotify).&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;lyrics&lt;/td&gt;
      &lt;td&gt;JSON&lt;/td&gt;
      &lt;td&gt;string&lt;/td&gt;
      &lt;td&gt;Lyrics queried from Genius (http://genius.com)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;lyric_table:&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Feature&lt;/th&gt;
      &lt;th&gt;SQL Type&lt;/th&gt;
      &lt;th&gt;Data Type&lt;/th&gt;
      &lt;th&gt;Description&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;track_id&lt;/td&gt;
      &lt;td&gt;VARCHAR / PRIMARY KEY&lt;/td&gt;
      &lt;td&gt;string&lt;/td&gt;
      &lt;td&gt;Unique track ID assigned by Spotify and used to trace back to song metadata throughout this project.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;rep_ratio&lt;/td&gt;
      &lt;td&gt;FLOAT&lt;/td&gt;
      &lt;td&gt;float&lt;/td&gt;
      &lt;td&gt;Ratio of unique words to total words in the track&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;total_words_track&lt;/td&gt;
      &lt;td&gt;INT&lt;/td&gt;
      &lt;td&gt;int&lt;/td&gt;
      &lt;td&gt;Word count for the track&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;unique_words_track&lt;/td&gt;
      &lt;td&gt;INT&lt;/td&gt;
      &lt;td&gt;int&lt;/td&gt;
      &lt;td&gt;Unique word count for the track&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;mean_len_words_track&lt;/td&gt;
      &lt;td&gt;FLOAT&lt;/td&gt;
      &lt;td&gt;float&lt;/td&gt;
      &lt;td&gt;Mean word length for the track&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;total_lines_track&lt;/td&gt;
      &lt;td&gt;INT&lt;/td&gt;
      &lt;td&gt;int&lt;/td&gt;
      &lt;td&gt;Line count for the track&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;unique_lines_track&lt;/td&gt;
      &lt;td&gt;INT&lt;/td&gt;
      &lt;td&gt;int&lt;/td&gt;
      &lt;td&gt;Unique line count for the track&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;mean_words_line&lt;/td&gt;
      &lt;td&gt;FLOAT&lt;/td&gt;
      &lt;td&gt;float&lt;/td&gt;
      &lt;td&gt;Mean word count per line in the track&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;mean_unique_words_line&lt;/td&gt;
      &lt;td&gt;FLOAT&lt;/td&gt;
      &lt;td&gt;float&lt;/td&gt;
      &lt;td&gt;Mean unique word count per line in the track&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;findings&quot;&gt;Findings&lt;a name=&quot;section5&quot;&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Creating a python-based word-level text generator, I decided to implement a sliding window sequence to feed data into the model. This means that an input sequence of &lt;em&gt;m&lt;/em&gt; words will be used to generate an output sequence of &lt;em&gt;n&lt;/em&gt; words. Sequencing the data in this way has the benefit of providing a ground truth feature that can be used to score model accuracy during training.&lt;/p&gt;

&lt;p&gt;Based on song structure findings during EDA, I decided on an input sequence of four words, an output sequence of one word, and a 250 word song length. This corresponds to an average of 9.1 words/line and 282.6 words/track seen in the dataset. I also chose to keep ‘\n’ characters in an effort to preserve song structure.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/words_line.png&quot; alt=&quot;mean words per line&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/words_track.png&quot; alt=&quot;words per track&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Based on track length, I decided to build my model with an embedding layer followed by a first LSTM layer of 300 nodes, corresponding to roughly 1 node per word in the track. This feeds into a second LSTM layer with 150 nodes, followed by a Dense layer with 100 nodes and finally an output layer with the same shape as the training corpus.&lt;/p&gt;

&lt;p&gt;The best results were produced when using a model trained over 300 epochs with batches of 5000 input sequences and untrained word word vectors with 3000 dimensions. The optimization function was the ADAM optimizer and the loss function was categorical cross-entropy.&lt;/p&gt;

&lt;p&gt;The model architecture was as follows:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Keras Embedding (3000)&lt;/li&gt;
  &lt;li&gt;LSTM (300, return_sequences=True)&lt;/li&gt;
  &lt;li&gt;LSTM (150)&lt;/li&gt;
  &lt;li&gt;Dense (100, ReLu)&lt;/li&gt;
  &lt;li&gt;Dense (vocab_size, softmax)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The Keras embedding layer is randomly initialized and fit alongside the model during training. Since I started with a vocabulary of 12334 unique words it seemed reasonable to begin with reducing the input dimensionality down to a 3000 feature dense representation. I also explored the use of pre-trained word vectors from the GloVe Common Crawl 42B dataset for comparison &lt;a href=&quot;https://nlp.stanford.edu/projects/glove/&quot;&gt;(source)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Lyrics produced from a model fit with pre-trained word vectors from the GloVe 42B dataset were more likely to repeat or fall into loops, requiring a larger variance coefficient during lyric generation to avoid repetition. This may indicated a need for more training epochs or a deeper overall model architecture. Since the embeddings were not a parameter being fit by the model, it is also possible that allowing the model to update embedding weights would also lead to better results. Line structure was also markedly less robust, often being choppy and 4-6 words compared to the longer lines of other models.&lt;/p&gt;

&lt;p&gt;To inject a tunable amount of variance in the model predictions, predicted probabilities for the next word were scaled and then randomly sampled from a distribution. This technique, adopted from the Keras LSTM example framework, prevents predictions from falling into a loop of high likelihood word patterns or into lyrics that the model was initially trained on &lt;a href=&quot;https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py&quot;&gt;(source)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This framework produced 250 word lyric sets that showed minimal repetition, maintained human readability, and preserved an output structure that could pass as real-world lyrics.&lt;/p&gt;

&lt;p&gt;Reviewing song structure as a repetition grid, here is the output for the model generated track from above:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/eyes_grid.png&quot; alt=&quot;eyes_grid&quot; /&gt; &lt;em&gt;Model-generated song repetition grid without pre-trained embeddings&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/always_grid.png&quot; alt=&quot;eyes_grid&quot; /&gt; &lt;em&gt;Model-generated song repetition grid with pre-trained embeddings&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/always_be.png&quot; alt=&quot;eyes_grid&quot; /&gt; &lt;em&gt;Lyrics generated from the seed ‘always be there love’&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In this grid, the vertical and horizontal axis represent each word in the track. If a word is repeated, the cell is filled, otherwise it is left white. This can be compared to a real track below. Note that the song below has about 30% higher word repetition (~ 0.64, average for the dataset compared to 0.39 above). It is possible that as the model becomes better optimized these structural components will become more well-defined.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/teddy_grid.png&quot; alt=&quot;real_grid&quot; /&gt;&lt;em&gt;Real song repetition grid&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/teddy.png&quot; alt=&quot;teddy&quot; /&gt;&lt;em&gt;Lyrics for When Somebody Loves You Back, Teddy Pendergrass &lt;a href=&quot;https://genius.com/Teddy-pendergrass-when-somebody-loves-you-back-lyrics&quot;&gt;(source)&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;business-recommendations&quot;&gt;Business Recommendations&lt;a name=&quot;section6&quot;&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Based on my findings, it’s clear that LSTM models provide a solid architecture for language processing and text generation. This model is robust enough to produce entertaining results that can be deployed in a consumer level product with minimal additional front-end development.&lt;/p&gt;

&lt;p&gt;This work provides a foundation for further exploration of word- and document-level language processing frameworks which can be applied to use-cases such as translation, marketing / copywriting, document summarization, image captioning, and other areas where automated speech and text may be beneficial.&lt;/p&gt;

&lt;p&gt;In the context of personalized, targeted marketing where brand voice and authenticity are paramount, a model such as this one may be able to provide basic copy that requires minimal human-in-the-loop feedback to become a market-ready document that adheres to pre-established and longstanding brand guidelines.&lt;/p&gt;

&lt;h2 id=&quot;next-steps&quot;&gt;Next steps&lt;a name=&quot;section7&quot;&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This model would benefit from a more exhaustive optimization of both architecture and hyperparameter tuning. To accomplish this will require scaling up compute.&lt;/p&gt;

&lt;p&gt;One tool that I would be interested in exploring is the Adanet adaptive architecture package that can dynamically test the addition of layers and nodes to create better predictions. I would have also liked to have explored pre-trained word vectors with longer training runs as I’m confident that a better semantic understanding of the data would result in more humanistic predictions.&lt;/p&gt;

&lt;p&gt;I would also like to build out the model with a more consumer-facing interface alongside the lyric repetition grid and interactive versions of other song structure plots produced during EDA.&lt;/p&gt;

&lt;p&gt;A natural extension of this project would be training on a larger dataset that is grouped by genre to see how well the model can mimic not only syntax, but also sentiment according to well conserved song structures.&lt;/p&gt;

&lt;p&gt;Another interesting idea would be training the model on company websites, press releases, and other public-facing documents to see how well the model can adopt brand voice.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;a name=&quot;section8&quot;&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;It is clear this model has great potential for language processing and text generation. For a word-level model to produce these results on a relatively small dataset is frankly quite remarkable. As an exploratory project seeking to demonstrate the potential of LSTM architectures for generating human-readable text, this is a tremendous starting point. Consumer and enterprise-level use-cases for this functionality are broad and there’s no doubt that machine-generated language processing will only become more prevalent in the coming years.&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;b&gt;Full project repo is available on &lt;a href=&quot;https://github.com/csinatra/portfolio/tree/master/crooner&quot; target=&quot;_blank&quot;&gt;GitHub&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 01 Feb 2019 00:00:00 -0800</pubDate>
        <link>https://chrissinatra.dev/2019/02/01/project-crooner/</link>
        <guid isPermaLink="true">https://chrissinatra.dev/2019/02/01/project-crooner/</guid>
        
        <category>projects</category>
        
        <category>LSTM</category>
        
        <category>RNN</category>
        
        <category>neural network</category>
        
        <category>NLP</category>
        
        <category>generative model</category>
        
        <category>spotify</category>
        
        <category>genius</category>
        
        
      </item>
    
      <item>
        <title>New Light Technologies: Disaster Locator</title>
        <description>&lt;p&gt;The client, Natural Light Technologies, is a FEMA contractor focusing on improving disaster detection, response, and resource allocation. This project focuses on leveraging Twitter data to identify the location of disaster survivors to allocate emergency response resources based on urgency. This has direct application to challenges encountered by response teams including: detecting a current emergency event, identifying social media keywords and hashtags to filter relevant content, ranking urgency based context, and using location data to direct rescue efforts.&lt;/p&gt;

&lt;h2 id=&quot;problem-statement&quot;&gt;Problem Statement&lt;/h2&gt;
&lt;p&gt;The core focus of this project is leveraging Twitter data to detect an emergency event, locate survivors, and direct emergency response resources based on urgency. Breaking this down, the main challenges are developing and applying an NLP driven sentiment analysis to separate emergencies from non-emergencies and rank by urgency, and to utilize location data to pinpoint those individuals in an easily accessible format. This project has the potential to streamline disaster response efforts, improving response time, increasing rescue efficiency, and reducing disaster related morbidity and mortality.&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;Twitter is a social media platform where users can post and interact with messages known as tweets. We searched tweets using a custom list of search terms relevant to disasters.&lt;/p&gt;

&lt;p&gt;Keywords are words or phrases which will be used to determine the relevance of a tweet and classify it. There are two classes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;On topic:&lt;/strong&gt; Tweets that contain keywords relevant to the current disaster.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Off topic:&lt;/strong&gt; Tweets that contain our search terms but are not referring to the disaster.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;data-dictionary&quot;&gt;Data Dictionary&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Feature&lt;/th&gt;
      &lt;th&gt;Data Type&lt;/th&gt;
      &lt;th&gt;Description&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;id&lt;/td&gt;
      &lt;td&gt;str&lt;/td&gt;
      &lt;td&gt;Unique Tweet id returned by the Twitter API&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;text&lt;/td&gt;
      &lt;td&gt;str&lt;/td&gt;
      &lt;td&gt;Text from the Tweet&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;label_on-topic&lt;/td&gt;
      &lt;td&gt;bool&lt;/td&gt;
      &lt;td&gt;Label based on relatedness to the disaster. Classes are ‘false’ for off-topic or ‘true’ for on-topic&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;disaster_type&lt;/td&gt;
      &lt;td&gt;str&lt;/td&gt;
      &lt;td&gt;Disaster type. Classes are ‘flood’, ‘hurricane’, and ‘tornado’&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;processed&lt;/td&gt;
      &lt;td&gt;str&lt;/td&gt;
      &lt;td&gt;Regex cleaned text mapping to lowercase, removing excess whitespace characters, removing links, and removing symbols.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;tokenized&lt;/td&gt;
      &lt;td&gt;str&lt;/td&gt;
      &lt;td&gt;List of words from the original Tweet split into single and 2-word tokens&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;lemmatized&lt;/td&gt;
      &lt;td&gt;str&lt;/td&gt;
      &lt;td&gt;List of tokens converted into their root lemmatized form&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;1. Data Collection&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Train&lt;/strong&gt;: From an external source, we gathered a labeled dataset to explore if we could correctly classify a natural disaster based on the contents of a tweet. &lt;br /&gt;
    (Source: &lt;a href=&quot;http://crisislex.org/data-collections.html#CrisisLexT6&quot;&gt;CrisisLex Disaster&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Test&lt;/strong&gt;: Using the Twitter standard tweet search API, we collected tweets using search terms relevant to current disasters and geo coordinates of current disasters. $Xnumber$ tweets were collected from January 6, 2019 - January 16, 2019. We extracted the text and bounding box coordinates.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. Exploratory Data Analysis and Cleaning&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For a better understanding of the data, we performed exploratory data analysis. With the text data, we exercised standard natural language processing techniques such as tokenizing and lemmatizing. Then we split the data into train and test dataset and performed TF-IDF vectorizing.&lt;/p&gt;

&lt;p&gt;We want to use TF-IDF to determine which words are most discriminating between tweets. Words that occur frequently are penalized and rare words are given more influence in our model.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The n-gram range was set with a lower bound of 1 and an upper bound of 2 so that word sequences contain at least one word and up to two words.&lt;/li&gt;
  &lt;li&gt;Filter out commonly used words in English.&lt;/li&gt;
  &lt;li&gt;Ignore terms that occur in less than 25 documents of the corpus.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;3. Preprocessing and Modeling&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Sandy Hurricane&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Baseline Model:
    &lt;ul&gt;
      &lt;li&gt;Accuracy score: 61.33%&lt;/li&gt;
      &lt;li&gt;If we classify all tweets as being on-topic, we will be predicting correctly 61.33% of the time.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Random Forest:
    &lt;ul&gt;
      &lt;li&gt;Accuracy score: 92.49%&lt;/li&gt;
      &lt;li&gt;The accuracy scores are greater than our baseline model of 61.33%.&lt;/li&gt;
      &lt;li&gt;The model is suffering from high variance, with our training score being roughly 4.45% higher than our test score.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Logistic Regression:
    &lt;ul&gt;
      &lt;li&gt;Accuracy score: 96.06%&lt;/li&gt;
      &lt;li&gt;The accuracy scores are greater than our baseline model of 61.33% and our Random Forest of 92.49%.&lt;/li&gt;
      &lt;li&gt;The model is still slightly overfitting to the train data because the variance explained from train data is 0.8% higher than the variance explained in the test data.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Oklahoma Tornado&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Baseline Model:
    &lt;ul&gt;
      &lt;li&gt;Accuracy score: 53.84%&lt;/li&gt;
      &lt;li&gt;If we classify all tweets as being on-topic, we will be predicting correctly 53.84% of the time.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Random Forest:
    &lt;ul&gt;
      &lt;li&gt;Accuracy score: 93.59%&lt;/li&gt;
      &lt;li&gt;The accuracy scores are greater than our baseline model of 53.83%.&lt;/li&gt;
      &lt;li&gt;The model is still slightly overfitting to the train data because the variance explained from train data is 4.15% higher than the variance explained in the test data.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Logistic Regression:
    &lt;ul&gt;
      &lt;li&gt;Accuracy score: 97.28%&lt;/li&gt;
      &lt;li&gt;The accuracy scores are greater than our baseline model of 53.83%.&lt;/li&gt;
      &lt;li&gt;The model is still slightly overfitting to the train data because the variance explained from train data is 1% higher than the variance explained in the test data.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Alberta Floods and Queensland Flood&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Baseline Model:
    &lt;ul&gt;
      &lt;li&gt;Accuracy score: 52.85%&lt;/li&gt;
      &lt;li&gt;If we classify all tweets as being on-topic, we will be predicting correctly 53.84% of the time.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Random Forest:
    &lt;ul&gt;
      &lt;li&gt;Accuracy score: 95.95%&lt;/li&gt;
      &lt;li&gt;The accuracy scores are greater than our baseline model of 53.83%.&lt;/li&gt;
      &lt;li&gt;The model is still slightly overfitting to the train data because the variance explained from train data is 2.61% higher than the variance explained in the test data.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Logistic Regression:
    &lt;ul&gt;
      &lt;li&gt;Accuracy score: 97.28%&lt;/li&gt;
      &lt;li&gt;The accuracy scores are greater than our baseline model of 53.83%.&lt;/li&gt;
      &lt;li&gt;The model is still slightly overfitting to the train data because the variance explained from train data is 1% higher than the variance explained in the test data.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;All Disasters&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Baseline Model:
    &lt;ul&gt;
      &lt;li&gt;Accuracy score: 53.84%&lt;/li&gt;
      &lt;li&gt;If we classify all tweets as being on-topic, we will be predicting correctly 53.84% of the time.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Random Forest:
    &lt;ul&gt;
      &lt;li&gt;Accuracy score: 94.42%&lt;/li&gt;
      &lt;li&gt;The accuracy scores are greater than our baseline model of 53.83%.&lt;/li&gt;
      &lt;li&gt;The model is still slightly overfitting to the train data because the variance explained from train data is 4.41% higher than the variance explained in the test data.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Logistic Regression:
    &lt;ul&gt;
      &lt;li&gt;Accuracy score: 98.03%&lt;/li&gt;
      &lt;li&gt;
        &lt;ul&gt;
          &lt;li&gt;The accuracy scores are greater than our baseline model of 53.84%.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;The model is still slightly overfitting to the train data because the variance explained from train data is 0.31% higher than the variance explained in the test data.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;4. Mapping Simulation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Geo-coordinates are randomly generated for tweets. These tweets are then mapped, with red representing &lt;code class=&quot;highlighter-rouge&quot;&gt;on-topic&lt;/code&gt; and blue representing &lt;code class=&quot;highlighter-rouge&quot;&gt;off-topic&lt;/code&gt; to better visualize disaster location.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;5. Proof of Concept&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We query tweets in Malibu, CA and Riverside, CA with flood-related search terms. We cleaned, preprocessed and modeled these raw tweets using the pipeline outlined in previous notebooks.&lt;/p&gt;

&lt;h2 id=&quot;model-comparison&quot;&gt;Model Comparison&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Advantages with Random Forest&lt;/strong&gt;: The model is more sophisticated and may perform better on less obvious data which can may mean less data preprocessing.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Advantages with Logisitic Regression&lt;/strong&gt;:
We can easily determine how likely a tweet will be classified as &lt;code class=&quot;highlighter-rouge&quot;&gt;on-topic&lt;/code&gt; by taking the exponential of the log odds of words. Even though Random Forest informs us about feature importance, we cannot quickly determine the information gained to classify a tweet as &lt;code class=&quot;highlighter-rouge&quot;&gt;on-topic&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;conclusions-and-recommendations&quot;&gt;Conclusions and Recommendations&lt;/h2&gt;
&lt;p&gt;Given a training set, we were able to classify disasters with high confidence based on the context of an individual tweet. This is important as given an unseen set of tweets, we are able to identify if a disaster is occurring. We randomly assigned geo coordinates to the training set to simulate a real-life scenario in which first responders would benefit from knowing the specific locations on a map to contain the disaster at hand.&lt;/p&gt;

&lt;p&gt;We simulated such process when we gathered real-life flood related tweets. Applying the previously built pipeline for our training data, we classified with high accuracy that a real tweet can be classified into its respective disaster category.&lt;/p&gt;

&lt;h2 id=&quot;next-steps&quot;&gt;Next Steps&lt;/h2&gt;

&lt;p&gt;There were three areas that we wanted to improve on:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lack of recent real world tweet data&lt;/strong&gt;: We were unable to find sufficient recent real world disaster tweets from which to test our trained models on. The raw data we were able to collect were not labeled as relevant or not which limited the models we wanted to use.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lack of real geo coordinates&lt;/strong&gt;: We were unable to gather real geo coordinates. Instead we had generated hypothetical geo coordinates of different tweets to mimic a natural disaster. Had we had access to the actual geo-coordinates, we could have accurately simulated a disaster through our plot.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lack of compute&lt;/strong&gt;: With more computing power, we could have scaled our analysis to gather more tweets and run our models handle more tweets and provide a more confident model.&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;b&gt;Full project repo is available on &lt;a href=&quot;https://github.com/csinatra/portfolio/tree/master/nlt_twitter&quot; target=&quot;_blank&quot;&gt;GitHub&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 01 Jan 2019 00:00:00 -0800</pubDate>
        <link>https://chrissinatra.dev/2019/01/01/project-nlt/</link>
        <guid isPermaLink="true">https://chrissinatra.dev/2019/01/01/project-nlt/</guid>
        
        <category>projects</category>
        
        <category>NLP</category>
        
        <category>n-grams</category>
        
        <category>Tf-idf</category>
        
        <category>Random Forest</category>
        
        <category>Topic modeling</category>
        
        <category>Classification</category>
        
        <category>Twitter</category>
        
        <category>Sentiment Analysis</category>
        
        <category>New Light Technologies</category>
        
        <category>Disaster</category>
        
        
      </item>
    
      <item>
        <title>Political Subgroup Identification on Reddit</title>
        <description>&lt;p&gt;Creating accurate classification models has application in assessing journalistic sources as well as optimizing communications to create a deeper connection with the target audience. To explore this space, I used Natural Language Processing techniques to identify linguistic differences between closely related but ideologically distinct political subgroups. In this case, the groups of interest are Conservatives and Libertarians.&lt;/p&gt;

&lt;p&gt;My model classified my data with 72.4% accuracy, showing a positive proof of concept. The best model was multinomial naive bayes, a relatively simple model that suggests there is room for significant improvement as I move toward more sophisticated models. The baseline accuracy for my dataset was 53.8%.&lt;/p&gt;

&lt;h2 id=&quot;problem-statement&quot;&gt;Problem Statement&lt;/h2&gt;
&lt;p&gt;The purpose of this project is to use Natural Language Processing techniques to identify linguistic differences between closely related but ideologically distinct political subgroups. In this case, the groups of interest are Conservatives and Libertarians. Creating accurate classification models has application in assessing journalistic sources as well as optimizing communications to create a deeper connection with the target audience.&lt;/p&gt;

&lt;h2 id=&quot;executive-summary&quot;&gt;Executive Summary&lt;/h2&gt;
&lt;p&gt;My model classified my data with 72.4% accuracy, showing a positive proof of concept. The best model was multinomial naive bayes, a relatively simple model that suggests there is room for significant improvement as I move toward more sophisticated models. The baseline accuracy was 53.8%&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;
&lt;p&gt;As access to information becomes more distributed across web sources the ability to verify journalistic integrity becomes a pressing concern. This is made clear by ongoing issues surrounding social media platforms’ promotion of ‘fake news’ designed to influence political outcomes as well as the broader public conversation around bias in media.&lt;/p&gt;

&lt;p&gt;This project aims to identify semantic and syntactical markers associated with two similar but distinct political subgroups: Conservatives and Libertarians. Understanding the ideological underpinnings that define communication dynamics within groups can be applied by modeling baseline tone for a given source and alerting audiences to significantly divergent texts. It may also be used for identifying bots or other inauthentic sources which may attempt to influence group sentiment.&lt;/p&gt;

&lt;p&gt;Outside of propaganda detection, accurate modeling may also be used for scoring articles or press releases for relevance among the target audience. By scoring words that hold weight within a group the communication may be optimized for relatability, impact, scope, and demographics.&lt;/p&gt;

&lt;p&gt;A text corpus will be built by querying Reddit and compiling comments from the Conservative and Libertarian subreddits that can be used for Supervised Learning using various classification models. The Pushshift API will be used to collect 100,000 samples from each subreddit and the data will be cleaned using Regex and word vectorization prior to Natural Language Processing (NLP). The clean data will be divided into training and validation sets before being fit by a given model before predicting classifications using the validation dataset. Models will include CART methods, Support Vector Machines (SVM), K-Nearest Neighbors / Latent Semantic Analysis (KNN / LSA), and others before determining a final production model.&lt;/p&gt;

&lt;h2 id=&quot;findings&quot;&gt;Findings&lt;/h2&gt;
&lt;p&gt;During EDA I found that Libertarians generally had higher word and character counts for a given document compared with Conservatives. While Count Vectorization produced similar top word lists, Tfidf Vectorization was able to identify words that had a stronger association with each groups individual ideology. As such, Tfidf data was used for modeling.&lt;/p&gt;

&lt;p&gt;Most frequent words for each category:
&lt;img src=&quot;/assets/images/reddit_nlp_frequency.svg&quot; alt=&quot;Word Frequencies&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Singular Value Decomposition, First 2 Components:
&lt;img src=&quot;/assets/images/reddit_nlp_SVD.svg&quot; alt=&quot;SVD&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Feature Importance:
&lt;img src=&quot;/assets/images/reddit_nlp_features.svg&quot; alt=&quot;SVD&quot; /&gt;&lt;/p&gt;

&lt;p&gt;My baseline prediction was 53.8% accuracy by predicting the positive class for all samples.&lt;/p&gt;

&lt;p&gt;The Random Forest model showed strong performance with the training data however it performed poorly on the validation data, showing signs of overfitting. The predictive accuracy on untrained data was only 70.5%, less than the naive bayes model. It’s likely this is was a result of using a small number of trees for fitting, and considering a larger number for each split would produce a more generalizable result.&lt;/p&gt;

&lt;p&gt;The Multinomial Naive Bayes model performed consistently between the training and validation datasets, predicting classes with 72.4% accuracy. Considering the simplicity fo the model and that the model was fit to Term Frequency / Inverse Document Frequency vectorized data this is a strong increase over the baseline model.&lt;/p&gt;

&lt;h2 id=&quot;recommendations&quot;&gt;Recommendations&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Fit additional models&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Use GridSearch to optimize over a larger space of hyperparameters&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;More aggressive data cleaning to streamline computations and reduce compute time.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;This preliminary investigation showed strong positive results and set the groundwork for further investigations. Continuing with a wider group of model algorithms as well as allocating more compute to tuning our models is likely to produce a generalizable model that predicts with a much higher accuracy. For initial studies, the Multinomial Naive Bayes may serve as the minimum viable product to put this classification system into production.&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;b&gt;Full project repo is available on &lt;a href=&quot;https://github.com/csinatra/portfolio/tree/master/reddit_NLP&quot; target=&quot;_blank&quot;&gt;GitHub&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 01 Jan 2019 00:00:00 -0800</pubDate>
        <link>https://chrissinatra.dev/2019/01/01/project-reddit_NLP/</link>
        <guid isPermaLink="true">https://chrissinatra.dev/2019/01/01/project-reddit_NLP/</guid>
        
        <category>NLP</category>
        
        <category>Reddit</category>
        
        <category>SVM</category>
        
        <category>KNN</category>
        
        <category>SCD</category>
        
        <category>classification</category>
        
        <category>multinomial naive bayes</category>
        
        
      </item>
    
  </channel>
</rss>
